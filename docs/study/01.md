# Day 01: 인터페이스와 데이터 파이프라인(공부)


## 1. 시스템 아키텍처
Data-Flux 프로젝트는 Spring Batch의 Chunk 지향 모델을 따르며 각 단계는 인터페이스를 통해 느슨하게 결합하는 형식을 지향함

1. Reader: 엑셀 원석 수급 (`DataPacket` 생성)
2. Processor: 원석 가공 및 분해 (1:24 시계열 분리)
3. Writer: 최종 제품 출고 (JDBC Bulk Insert로 DB 적재)

---

## 2. 핵심 부품과 인터페이스의 역할

### ① ItemReader (데이터를 하나씩 공급)
- 구현체: `ExcelItemReader`
- 핵심 기술: Apache POI + Iterator
- 설계 의도* 엑셀 전체를 메모리에 올리지 않고 `Iterator`를 통해 한 줄씩 읽어 메모리 점유율(Heap)**을 최소화 `null`을 반환하여 배치의 종료(EOF)를 알림

### ② ItemProcessor (A를 받아 B로 변환)
- 구현체: `DataDecompositionProcessor`
- 핵심 기술: Java 8 Time API + 1:N Mapping
- 설계 의도: 비즈니스 로직의 핵심인 '데이터 분해'를 별도 계층으로 격리 / 입력 규격(`DataPacket`)과 출력 규격(`List<DataPoint>`)만 지키면 내부 로직이 바뀌어도 전체 공정에 영향이 없음

### ③ ItemWriter (뭉쳐서 한꺼번에 저장)
- 구현체: `BulkDataWriter`
- 핵심 기술: `JdbcTemplate` + Batch Update
- 설계 의도: JPA의 성능 한계(Identity 전략의 Batch Insert 불가)를 극복하기 위해 기술적 수준을 낮춰(Low-level) 직접 SQL을 핸들링

---

## 3. 리스크 관리 (Fault Tolerance)
`DataIngestJobConfig`에서 조립 시 적용된 안전장치들:

- Chunk(N): 트랜잭션 범위를 100개로 제한하여 메모리 압박 및 DB 락(Lock) 점유 최소화
- Skip: 부적절한 데이터(IllegalArgumentException) 발생 시 해당 건만 버리고 공정 유지 (최대 N건)
- Retry: DB 데드락 등 일시적 오류 시 3회 재시도하여 배치 성공률 제고

---

## 복습내용
1. 인터페이스의 본질:<br/>
추상화의 가치: ItemProcessor 인터페이스의 process() 메서드는 단 한 줄이지만 이는 프레임워크(Spring Batch)와 내 코드 사이의 강력한 계약서<br/>
느슨한 결합: 프레임워크는 내 로직의 상세 구현(엑셀 분해 로직)을 몰라도 process()라는 규격만 보고 데이터를 넘겨준다. 덕분에 비즈니스 로직이 변경되어도 전체 시스템 아키텍처는 무너지지 않는다.  

2. 데이터 흐름 분석:<br/>
현재 Data-Flux 프로젝트의 공정 라인은 다음과 같은 단방향 흐름을 가진다.<br/>
Reader (ExcelItemReader): 외부 원천 데이터(Excel)를 자바 객체(DataPacket)로 정규화하여 시스템 내부로 수급<br/>
Processor (DataDecompositionProcessor): 1개 행을 24개의 시계열 레코드로 분해하는 핵심 비즈니스 로직 수행<br/>
Writer (BulkDataWriter): 가공된 데이터를 JdbcTemplate을 통해 DB 엔진에 벌크 적재<br/>

3. 개선 내용<br/>
JDBC Bulk Insert 선택: JPA IDENTITY 전략의 성능 한계를 인지하고 추상화 계층을 한 단계 내려 JDBC를 직접 사용 이를 통해 네트워크 왕복 비용을 획기적으로 줄여 적재 속도를 개선<br/>
안정성 확보: Chunk 단위를 통한 트랜잭션 격리와 Skip/Retry 정책을 통해 대용량 처리 중 발생할 수 있는 데이터 오류 및 일시적 장애(Deadlock 등)에 대응

---
