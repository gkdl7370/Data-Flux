# Day 01: 인터페이스와 데이터 파이프라인(공부)


## 1. 시스템 아키텍처
Data-Flux 프로젝트는 Spring Batch의 Chunk 지향 모델을 따르며 각 단계는 인터페이스를 통해 느슨하게 결합하는 형식을 지향함

1. Reader: 엑셀 원석 수급 (`DataPacket` 생성)
2. Processor: 원석 가공 및 분해 (1:24 시계열 분리)
3. Writer: 최종 제품 출고 (JDBC Bulk Insert로 DB 적재)

---

## 2. 핵심 부품과 인터페이스의 역할

### ① ItemReader (데이터를 하나씩 공급)
- 구현체: `ExcelItemReader`
- 핵심 기술: Apache POI + Iterator
- 설계 의도* 엑셀 전체를 메모리에 올리지 않고 `Iterator`를 통해 한 줄씩 읽어 메모리 점유율(Heap)**을 최소화 `null`을 반환하여 배치의 종료(EOF)를 알림

### ② ItemProcessor (A를 받아 B로 변환)
- 구현체: `DataDecompositionProcessor`
- 핵심 기술: Java 8 Time API + 1:N Mapping
- 설계 의도: 비즈니스 로직의 핵심인 '데이터 분해'를 별도 계층으로 격리 / 입력 규격(`DataPacket`)과 출력 규격(`List<DataPoint>`)만 지키면 내부 로직이 바뀌어도 전체 공정에 영향이 없음

### ③ ItemWriter (뭉쳐서 한꺼번에 저장)
- 구현체: `BulkDataWriter`
- 핵심 기술: `JdbcTemplate` + Batch Update
- 설계 의도: JPA의 성능 한계(Identity 전략의 Batch Insert 불가)를 극복하기 위해 기술적 수준을 낮춰(Low-level) 직접 SQL을 핸들링

---

## 3. 리스크 관리 (Fault Tolerance)
`DataIngestJobConfig`에서 조립 시 적용된 안전장치들:

- Chunk(N): 트랜잭션 범위를 100개로 제한하여 메모리 압박 및 DB 락(Lock) 점유 최소화
- Skip: 부적절한 데이터(IllegalArgumentException) 발생 시 해당 건만 버리고 공정 유지 (최대 N건)
- Retry: DB 데드락 등 일시적 오류 시 3회 재시도하여 배치 성공률 제고

---

# 복습내용

1. 인터페이스의 본질:<br/>
   추상화의 가치: ItemProcessor 인터페이스의 process() 메서드는 단 한 줄이지만 이는 프레임워크(Spring Batch)와 내 코드 사이의 강력한 계약서<br/>
   느슨한 결합: 프레임워크는 내 로직의 상세 구현(엑셀 분해 로직)을 몰라도 process()라는 규격만 보고 데이터를 넘겨준다. 덕분에 비즈니스 로직이 변경되어도 전체 시스템 아키텍처는 무너지지 않는다.<br/>

2. 데이터 흐름 분석:<br/>
   현재 Data-Flux 프로젝트의 공정 라인은 다음과 같은 단방향 흐름을 가진다.<br/>
   Reader (ExcelItemReader): 외부 원천 데이터(Excel)를 자바 객체(DataPacket)로 정규화하여 시스템 내부로 수급<br/>
   Processor (DataDecompositionProcessor): 1개 행을 24개의 시계열 레코드로 분해하는 핵심 비즈니스 로직 수행<br/>
   Writer (BulkDataWriter): 가공된 데이터를 JdbcTemplate을 통해 DB 엔진에 벌크 적재<br/>

3. 트랜잭션 관리:<br/>
   Chunk Size()는 곧 트랜잭션의 단위이다. 데이터 정합성을 위해 장애 발생 시 해당 청크 전체가 롤백되는 메커니즘을 고려하여 설계해야 한다.<br/>

4. 스트림 API 활용:<br/>
   `BulkDataWriter` 에서 중첩 리스트를 평탄화할 때 Java Stream을 사용하여 가독성을 높였으나 향후 성능 병목 발생 시 전통적 루프로의 최적화 가능성을 열어둠<br/>

5. 확장성:<br/>
   현재는 Excel-to-DB 구조이나 인터페이스 기반 설계를 통해 향후 Kafka나 API 수급 방식으로 확장할 수 있는 'Data Flux' 아키텍처의 기반을 마련해볼 생각<br/>

6. JDBC Bulk Insert 선택:<br/>
   JPA의 IDENTITY 전략은 새로운 레코드의 ID를 확인하기 위해 건별로 INSERT를 수행해야 하므로 네트워크 왕복 비용이 기하급수적으로 증가하는 문제가 있음(100개 넣으려면 100번 왔다갔다)<br/>
   saveAll()을 써도 내부 적으로는 한 건씩 100번 날림<br/>

8. 그럼 무조건 JDBC Bulk Insert가 좋은가?<br/>
   배치(Batch)처럼 대량 처리가 핵심인 곳에는 JDBC Bulk가 좋고 일반적인 웹 서비스(API)에서는 JPA가 유리<br/>
   JPA는 save(entity) 한 줄이면 끝나지만 JDBC는 SQL 문을 직접 작성하고 파라미터를 일일이 매핑<br/>
   테이블 컬럼이 하나만 추가되어도 SQL 문과 자바의 매핑 코드를 모두 수정해야 하지만 JPA는 필드 하나만 추가하면 끝남<br/>

10. 안정성 확보:<br/>
   Chunk 단위를 통한 트랜잭션 격리와 Skip/Retry 정책을 통해 대용량 처리 중 발생할 수 있는 데이터 오류 및 일시적 장애(Deadlock 등)에 대응

---
